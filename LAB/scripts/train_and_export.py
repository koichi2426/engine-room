#!/usr/bin/env python3
# ============================================
# TinyBERT / TinyBERT-4L ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# 1. Triplet Loss ã«ã‚ˆã‚‹æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«å­¦ç¿’ (SBERTæ§‹é€ )
# 2. ONNXå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆFP32ï¼‰
# 3. INT8é‡å­åŒ–ï¼ˆã‚¨ãƒƒã‚¸æ¨è«–å‘ã‘ï¼‰
# 4. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—ï¼ˆpretrainedå‡ºåŠ›ï¼‰ãƒ¢ãƒ¼ãƒ‰ã«ã‚‚å¯¾å¿œ
# ============================================

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType
import numpy as np
from tqdm import tqdm

# ==========================
# ãƒ¢ãƒ‡ãƒ«é¸æŠ
# ==========================
def select_model():
    print("\nğŸ§  å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:\n")
    print("  [1] prajjwal1/bert-tiny  (2å±¤, hidden=128, è¶…è»½é‡)")
    print("  [2] huawei-noah/TinyBERT_General_4L_312D  (4å±¤, hidden=312, é«˜ç²¾åº¦)\n")

    choice = input("ğŸ‘‰ ãƒ¢ãƒ‡ãƒ«ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [1/2]: ").strip()

    if choice == "2":
        model_name = "huawei-noah/TinyBERT_General_4L_312D"
        base_dir = "models/TinyBERT_General_4L_312D"
        finetuned_dir = "finetuned_models/TinyBERT_General_4L_312D"
    else:
        model_name = "prajjwal1/bert-tiny"
        base_dir = "models/bert-tiny"
        finetuned_dir = "finetuned_models/bert-tiny"

    os.makedirs(finetuned_dir, exist_ok=True)
    return model_name, base_dir, finetuned_dir


# ==========================
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã‹é¸æŠ
# ==========================
def ask_finetune():
    print("\nğŸ§© ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ")
    print("  [1] ã¯ã„ï¼ˆTriplet Loss ã«ã‚ˆã‚‹å­¦ç¿’ã‚’å®Ÿè¡Œï¼‰")
    print("  [2] ã„ã„ãˆï¼ˆpretrained ãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼‰\n")
    return input("ğŸ‘‰ é¸æŠã—ã¦ãã ã•ã„ [1/2]: ").strip() == "1"


# ==========================
# å…±é€šè¨­å®š
# ==========================
TRAIN_DATA_PATH = "data/train_triplets.txt"
MAX_LENGTH = 32
BATCH_SIZE = 16
EPOCHS = 5
LR = 2e-5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# ==========================
# SBERTæ§‹é€ ï¼ˆmean poolingï¼‰
# ==========================
class SBERTEncoder(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = output.last_hidden_state
        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
        summed = (last_hidden * mask).sum(1)
        counts = mask.sum(1)
        mean_pooled = summed / counts
        return mean_pooled


# ==========================
# Tripletãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ==========================
class TripletDataset(Dataset):
    def __init__(self, path, tokenizer):
        self.samples = []
        with open(path, encoding="utf-8") as f:
            for line in f:
                anchor, pos, neg = line.strip().split("\t")
                self.samples.append((anchor, pos, neg))
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        a, p, n = self.samples[idx]
        return self.tokenizer(
            [a, p, n],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH
        )


# ==========================
# Triplet Loss
# ==========================
def triplet_loss(anchor, positive, negative, margin=1.0):
    d_ap = (anchor - positive).pow(2).sum(1)
    d_an = (anchor - negative).pow(2).sum(1)
    return torch.relu(d_ap - d_an + margin).mean()


# ==========================
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
# ==========================
def finetune_model(model_name, finetuned_dir):
    print("\n[1] ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    base_model = AutoModel.from_pretrained(model_name)
    model = SBERTEncoder(base_model).to(DEVICE)

    dataset = TripletDataset(TRAIN_DATA_PATH, tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)

    print("[2] ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹...")
    model.train()
    for epoch in range(EPOCHS):
        losses = []
        for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
            input_ids = batch["input_ids"].squeeze(1).to(DEVICE)
            attention_mask = batch["attention_mask"].squeeze(1).to(DEVICE)
            a, p, n = input_ids[:, 0, :], input_ids[:, 1, :], input_ids[:, 2, :]
            am, pm, nm = attention_mask[:, 0, :], attention_mask[:, 1, :], attention_mask[:, 2, :]
            va, vp, vn = model(a, am), model(p, pm), model(n, nm)
            loss = triplet_loss(va, vp, vn)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            losses.append(loss.item())

        print(f"  âœ… Epoch {epoch+1}/{EPOCHS}  Loss: {np.mean(losses):.4f}")

    print("\nâœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†")

    model.bert.save_pretrained(finetuned_dir)
    tokenizer.save_pretrained(finetuned_dir)
    torch.save(model.state_dict(), os.path.join(finetuned_dir, "pytorch_model.bin"))
    return tokenizer, model


# ==========================
# ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==========================
def export_onnx(model, tokenizer, output_dir):
    print("\n[3] ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    model.eval()

    onnx_fp32 = os.path.join(output_dir, "model_fp32.onnx")
    dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, MAX_LENGTH), dtype=torch.long)
    dummy_attention_mask = torch.ones((1, MAX_LENGTH), dtype=torch.long)

    torch.onnx.export(
        model,
        (dummy_input_ids, dummy_attention_mask),
        onnx_fp32,
        input_names=["input_ids", "attention_mask"],
        output_names=["pooled_output"],
        dynamic_axes={"input_ids": {0: "batch"}, "attention_mask": {0: "batch"}},
        opset_version=13
    )
    print(f"âœ… ONNXãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å®Œäº† â†’ {onnx_fp32}")
    return onnx_fp32


# ==========================
# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
# ==========================
class DummyCalibReader(CalibrationDataReader):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.datas = [
            tokenizer(
                "ç½å®³ç™ºç”Ÿæ™‚ã«é¿é›£ã‚’ä¿ƒã™æ–‡ç« ã€‚",
                return_tensors="np",
                padding="max_length",
                truncation=True,
                max_length=MAX_LENGTH
            )
        ]
        self.index = 0

    def get_next(self):
        if self.index < len(self.datas):
            data = self.datas[self.index]
            self.index += 1
            return {
                "input_ids": data["input_ids"],
                "attention_mask": data["attention_mask"]
            }
        return None


# ==========================
# é‡å­åŒ–
# ==========================
def quantize_model(tokenizer, onnx_fp32, output_dir):
    print("\n[4] INT8é‡å­åŒ–ã‚’å®Ÿè¡Œä¸­...")
    onnx_int8 = os.path.join(output_dir, "model_int8.onnx")
    quantize_static(
        model_input=onnx_fp32,
        model_output=onnx_int8,
        calibration_data_reader=DummyCalibReader(tokenizer),
        quant_format=QuantType.QUInt8
    )
    print(f"âœ… INT8é‡å­åŒ–å®Œäº† â†’ {onnx_int8}")


# ==========================
# ãƒ¡ã‚¤ãƒ³
# ==========================
if __name__ == "__main__":
    model_name, base_dir, finetuned_dir = select_model()
    do_ft = ask_finetune()

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    base_model = AutoModel.from_pretrained(model_name)
    model = SBERTEncoder(base_model).to(DEVICE)

    # ---- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†å ´åˆ ----
    if do_ft:
        tokenizer, model = finetune_model(model_name, finetuned_dir)
        output_dir = finetuned_dir
        print("\nâœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚")

    # ---- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã‚ãªã„å ´åˆ ----
    else:
        output_dir = base_dir
        os.makedirs(output_dir, exist_ok=True)
        print("\nâœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãªã—: pretrainedãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾å‡ºåŠ›ã—ã¾ã™ã€‚")

    # ---- ONNX + é‡å­åŒ–å…±é€šå‡¦ç† ----
    onnx_fp32 = export_onnx(model, tokenizer, output_dir)
    quantize_model(tokenizer, onnx_fp32, output_dir)

    print("\nğŸ¯ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
