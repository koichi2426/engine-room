# ============================================
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# 1. methods.txt ã«æ›¸ã‹ã‚ŒãŸå„æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’èª­ã¿è¾¼ã‚€
# 2. é¸æŠã—ãŸONNXãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦å„ãƒ¡ã‚½ãƒƒãƒ‰æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
# 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚’ .npy å½¢å¼ã§ä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰
# 4. å…ƒãƒ†ã‚­ã‚¹ãƒˆã‚’ .json å½¢å¼ã§ä¿å­˜ï¼ˆè¡¨ç¤ºãƒ»æ¤œç´¢ç”¨ï¼‰
# ============================================

import os
import json
import numpy as np
from transformers import AutoTokenizer
import onnxruntime as ort


# ==========================
# ãƒ¢ãƒ‡ãƒ«é¸æŠé–¢æ•°ï¼ˆ4ç¨®é¡å¯¾å¿œï¼‰
# ==========================
def select_model():
    print("\nğŸ§  ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:\n")
    print("  [1] bert-tinyï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰")
    print("  [2] finetuned bert-tinyï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ï¼‰")
    print("  [3] TinyBERT_General_4L_312Dï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰")
    print("  [4] finetuned TinyBERT_General_4L_312Dï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ï¼‰\n")

    choice = input("ğŸ‘‰ ãƒ¢ãƒ‡ãƒ«ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [1-4]: ").strip()

    # --- é¸æŠã”ã¨ã®ãƒ‘ã‚¹è¨­å®š ---
    if choice == "1":
        model_dir = "models/bert-tiny"
        model_tag = "bert-tiny_pre"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: bert-tinyï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    elif choice == "2":
        model_dir = "finetuned_models/bert-tiny"
        model_tag = "bert-tiny_ft"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ bert-tiny ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    elif choice == "3":
        model_dir = "models/TinyBERT_General_4L_312D"
        model_tag = "TinyBERT_4L_pre"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: TinyBERTï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    else:
        model_dir = "finetuned_models/TinyBERT_General_4L_312D"
        model_tag = "TinyBERT_4L_ft"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ TinyBERT (4å±¤) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    tokenizer_path = model_dir
    return model_path, tokenizer_path, model_tag


# ==========================
# è¨­å®š
# ==========================
METHODS_TEXT_PATH = "data/methods.txt"
MAX_LENGTH = 32


# ==========================
# å˜ä¸€æ–‡ã‚’ONNXãƒ¢ãƒ‡ãƒ«ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
# ==========================
def encode(text, tokenizer, session):
    inputs = tokenizer(
        text,
        return_tensors="np",
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH
    )

    # âœ… ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ä»•æ§˜ã«åˆã‚ã›ã¦è‡ªå‹•èª¿æ•´
    valid_input_names = {i.name for i in session.get_inputs()}
    ort_inputs = {k: v for k, v in inputs.items() if k in valid_input_names}

    expected_types = {i.name: i.type for i in session.get_inputs()}
    for k, v in ort_inputs.items():
        if "int64" in expected_types.get(k, ""):
            ort_inputs[k] = v.astype("int64")
        elif "int32" in expected_types.get(k, ""):
            ort_inputs[k] = v.astype("int32")
        elif "float" in expected_types.get(k, ""):
            ort_inputs[k] = v.astype("float32")

    return session.run(["pooled_output"], ort_inputs)[0][0]


# ==========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================
def main():
    model_path, tokenizer_path, model_tag = select_model()

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    session = ort.InferenceSession(model_path)

    os.makedirs("data", exist_ok=True)

    with open(METHODS_TEXT_PATH, encoding="utf-8") as f:
        method_lines = [line.strip() for line in f if line.strip()]

    print(f"[1] {len(method_lines)}ä»¶ã®æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­ ({model_tag})...")

    vectors = np.array([encode(line, tokenizer, session) for line in method_lines])

    output_vec_path = f"data/method_vectors_{model_tag}.npy"
    output_text_path = f"data/method_texts_{model_tag}.json"

    np.save(output_vec_path, vectors)
    print(f"[2] ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {output_vec_path}")

    with open(output_text_path, "w", encoding="utf-8") as jf:
        json.dump(method_lines, jf, ensure_ascii=False, indent=2)
    print(f"[3] ãƒ¡ã‚½ãƒƒãƒ‰åŸæ–‡ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {output_text_path}")


# ==========================
# å®Ÿè¡Œ
# ==========================
if __name__ == "__main__":
    main()
