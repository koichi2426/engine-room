# ================================================
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼š
# 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›æ–‡ã‚’æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆONNXæ¨è«–ï¼‰
# 2. äº‹å‰ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤ã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
# 3. æœ€ã‚‚è¿‘ã„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’1ã¤é¸æŠã—ã¦è¿”ã™
# ================================================

import numpy as np
import onnxruntime as ort
from transformers import AutoTokenizer
import json
import os


# ==========================
# ãƒ¢ãƒ‡ãƒ«é¸æŠé–¢æ•°
# ==========================
def select_model():
    print("\nğŸ§  ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:\n")
    print("  [1] bert-tinyï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰")
    print("  [2] finetuned bert-tinyï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ï¼‰")
    print("  [3] TinyBERT_General_4L_312Dï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰")
    print("  [4] finetuned TinyBERT_General_4L_312Dï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ï¼‰\n")

    choice = input("ğŸ‘‰ ãƒ¢ãƒ‡ãƒ«ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [1-4]: ").strip()

    # --- ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ãƒ‘ã‚¹è¨­å®š ---
    if choice == "1":
        model_dir = "models/bert-tiny"
        model_tag = "bert-tiny_pre"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: bert-tinyï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    elif choice == "2":
        model_dir = "finetuned_models/bert-tiny"
        model_tag = "bert-tiny_ft"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ bert-tiny ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    elif choice == "3":
        model_dir = "models/TinyBERT_General_4L_312D"
        model_tag = "TinyBERT_4L_pre"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: TinyBERTï¼ˆæœªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    else:
        model_dir = "finetuned_models/TinyBERT_General_4L_312D"
        model_tag = "TinyBERT_4L_ft"
        model_path = os.path.join(model_dir, "model_int8.onnx")
        print(f"\nâœ… é¸æŠ: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ TinyBERT (4å±¤) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n")

    tokenizer_path = model_dir
    return model_path, tokenizer_path, model_tag


# ==========================
# å…¥åŠ›æ–‡ â†’ æ„å‘³ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆONNXæ¨è«–ï¼‰
# ==========================
def get_embedding(text, tokenizer, session, max_length=32):
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    inputs = tokenizer(
        text,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="np"
    )

    # âœ… ãƒ¢ãƒ‡ãƒ«ãŒè¦æ±‚ã™ã‚‹å…¥åŠ›åã®ã¿æŠ½å‡ºï¼ˆtoken_type_idsã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰
    valid_input_names = {i.name for i in session.get_inputs()}
    ort_inputs = {k: v for k, v in inputs.items() if k in valid_input_names}

    # âœ… ãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹dtypeã«åˆã‚ã›ã¦è‡ªå‹•ã‚­ãƒ£ã‚¹ãƒˆ
    expected_types = {i.name: i.type for i in session.get_inputs()}
    for k, v in ort_inputs.items():
        if "int64" in expected_types.get(k, ""):
            ort_inputs[k] = v.astype("int64")
        elif "int32" in expected_types.get(k, ""):
            ort_inputs[k] = v.astype("int32")
        elif "float" in expected_types.get(k, ""):
            ort_inputs[k] = v.astype("float32")

    # æ¨è«–å®Ÿè¡Œ
    embedding = session.run(["pooled_output"], ort_inputs)[0]
    return embedding[0]


# ==========================
# ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—
# ==========================
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


# ==========================
# ãƒ¡ã‚½ãƒƒãƒ‰é¸æŠï¼ˆé¡ä¼¼åº¦é †ï¼‰
# ==========================
def select_best_method(user_text):
    # [1] ãƒ¢ãƒ‡ãƒ«é¸æŠ
    model_path, tokenizer_path, model_tag = select_model()

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

    # [2] ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    session = ort.InferenceSession(model_path)

    # [3] å…¥åŠ›æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    user_vec = get_embedding(user_text, tokenizer, session)

    # [4] ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    METHODS_VECTORS_PATH = f"data/method_vectors_{model_tag}.npy"
    METHODS_TEXTS_PATH = f"data/method_texts_{model_tag}.json"

    if not os.path.exists(METHODS_VECTORS_PATH):
        raise FileNotFoundError(
            f"âŒ ãƒ™ã‚¯ãƒˆãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {METHODS_VECTORS_PATH}\nâ†’ å…ˆã« gen_method_vectors.py ã‚’ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        )

    method_vecs = np.load(METHODS_VECTORS_PATH)
    with open(METHODS_TEXTS_PATH, "r", encoding="utf-8") as f:
        method_texts = json.load(f)

    # [5] é¡ä¼¼åº¦è¨ˆç®—
    sims = [cosine_similarity(user_vec, v) for v in method_vecs]
    sorted_indices = np.argsort(sims)[::-1]

    # [6] çµæœæ•´å½¢
    results = [{"method": method_texts[i], "score": float(sims[i])} for i in sorted_indices]
    return results


# ==========================
# å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
# ==========================
if __name__ == "__main__":
    user_input = "I just walked past my favorite ramen shop"
    results = select_best_method(user_input)

    print("\n[ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›]", user_input)
    print("\n[é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢é †ãƒ¡ã‚½ãƒƒãƒ‰ä¸€è¦§]")
    for i, r in enumerate(results, 1):
        print(f"{i}. ã‚¹ã‚³ã‚¢: {r['score']:.4f}")
        print(f"   ãƒ¡ã‚½ãƒƒãƒ‰: {r['method']}")
        print()
